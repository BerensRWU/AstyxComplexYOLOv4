{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution and Maxpooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx2d = nn.MaxPool2d((2,2))\n",
    "cnn = nn.Conv2d(1,2,(2,2),dtype=torch.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(0,10,(1,4,4))\n",
    "x = torch.from_numpy(x)\n",
    "y = torch.arange(16, dtype=torch.float64).reshape(1,4,4)\n",
    "ones = torch.ones_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx2d(x),mx2d(y),mx2d(ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn(x),cnn(y),cnn(ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"/home/berens/remote/astyx/dataset/dataset_astyx_hires2019/camera_front/000000.jpg\")[:,:,[2,1,0]]\n",
    "img = img.swapaxes(0,2)\n",
    "img = torch.from_numpy(img).type(torch.float64)/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img.swapaxes(0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx2d= nn.MaxPool2d((10,10))\n",
    "cnn = nn.Conv2d(3,3,(10,10),dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_mx = mx2d(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_cnn = cnn(img)\n",
    "img_cnn = img_cnn.detach().numpy()\n",
    "img_cnn -= img_cnn.min()\n",
    "img_cnn /= img_cnn.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_mx.swapaxes(0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_cnn.swapaxes(0,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sequential model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Add a convolutional layer with 6 filters, a kernel size of 5x5 \n",
    "        self.conv1 = nn.Conv2d(3, 8, 5)\n",
    "        # Add a max pooling layer with a pool size of 2x2\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(8, 32, 5)\n",
    "        # Add a fully connected layer\n",
    "        self.fc1 = nn.Linear(32 * 5 * 5, 124)\n",
    "        self.fc2 = nn.Linear(124, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the CIFAR10 dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Resize((32,32)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classes\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Define the criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "print('Start Training')\n",
    "# Train the network\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    print('Epoch %d loss: %.3f' % (epoch + 1, running_loss / (i + 1)))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the network on the test data\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os, sys\n",
    "from easydict import EasyDict as edict\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "sys.path.append(\"./src/\")\n",
    "\n",
    "from src.models.model_utils import create_model, get_num_parameters\n",
    "from src.utils.evaluation_utils import get_batch_statistics_rotated_bbox, ap_per_class, post_processing_v2, rescale_boxes\n",
    "from src.data_process_astyx.astyx_dataloader import create_val_dataloader\n",
    "from src.data_process_astyx import astyx_bev_utils as bev_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequirements\n",
    "\n",
    "* configs - has all necassary information\n",
    "* val_dataloader - contains the dataset\n",
    "* model - contains the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoints/complex_yolov4_astyx_lidar_split_old/Model_complex_yolov4_astyx_lidar_split_old_epoch_180.pth\n",
    "#checkpoints/complex_yolov4_astyx_radar_VR_split_old/Model_complex_yolov4_astyx_radar_VR_split_old_epoch_100.pth\n",
    "#checkpoints/complex_yolov4_astyx_radar_VR_split_old/Model_complex_yolov4_astyx_radar_VR_split_old_epoch_130.pth\n",
    "#checkpoints/complex_yolov4_astyx_low_fusion_VR_split_old/Model_complex_yolov4_astyx_low_fusion_VR_split_old_epoch_130.pth\n",
    "#checkpoints/complex_yolov4_astyx_low_fusion_Mag_split_old/Model_complex_yolov4_astyx_low_fusion_Mag_split_old_epoch_700.pth\n",
    "\n",
    "configs = edict({\n",
    "                \"cfgfile\": \"./src/config/cfg/complex_yolov4.cfg\",\n",
    "                \"dataset_dir\": \"../astyx/dataset/dataset_astyx_hires2019/\",\n",
    "                \"pretrained_path\": \"checkpoints/complex_yolov4_astyx_low_fusion_VR_split_old/Model_complex_yolov4_astyx_low_fusion_VR_split_old_epoch_130.pth\",\n",
    "                \"radar\": False,\n",
    "                \"low_fusion\": True,\n",
    "                \"lidar\": False,\n",
    "                \"VR\": True,\n",
    "                \"mag\": False,\n",
    "                \"img_size\": 608,\n",
    "                \"conf_thresh\":0.5,\n",
    "                \"nms_thresh\":0.5,\n",
    "                \"iou_thresh\":0.5,\n",
    "                \"batch_size\": 1,\n",
    "                \"num_workers\": 1,\n",
    "                \"pin_memory\": True,\n",
    "                \"num_samples\": None,\n",
    "                \"device\": torch.device('cpu'),\n",
    "                \"arch\": \"darknet\",\n",
    "                \"use_giou_loss\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "val_dataloader = create_val_dataloader(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(configs)\n",
    "model.load_state_dict(torch.load(configs.pretrained_path, map_location=torch.device(configs.device)))\n",
    "model = model.to(device=configs.device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"The model has {get_num_parameters(model)} many parameters.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative Evaluation\n",
    "First we will visualy evaluate the model.\n",
    "\n",
    "For this we will load a sample and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_id = 25\n",
    "img_paths, imgs_bev, targets = val_dataloader.dataset[batch_id]\n",
    "input_imgs = imgs_bev.to(device=configs.device).float()\n",
    "input_imgs = torch.unsqueeze(input_imgs, dim=0)\n",
    "targets = torch.unsqueeze(targets, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(input_imgs)\n",
    "detections = post_processing_v2(outputs, conf_thresh=configs.conf_thresh, nms_thresh=configs.nms_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_detections = []  # Stores detections for each image index\n",
    "img_detections.extend(detections)\n",
    "\n",
    "img_bev = imgs_bev.squeeze() * 255\n",
    "img_bev = img_bev.permute(1, 2, 0).numpy().astype(np.uint8)\n",
    "img_bev = cv2.resize(img_bev, (configs.img_size, configs.img_size))\n",
    "\n",
    "img_bev[1:] = np.max(np.concatenate([img_bev[1:], img_bev[:-1]]).reshape(2,configs.img_size-1, configs.img_size,3),0)\n",
    "img_bev[2:] = np.max(np.concatenate([img_bev[2:], img_bev[:-2]]).reshape(2,configs.img_size-2, configs.img_size,3),0)\n",
    "img_bev[:,1:] = np.max(np.concatenate([img_bev[:,1:], img_bev[:,:-1]]).reshape(2,configs.img_size, configs.img_size-1,3),0)\n",
    "img_bev[:,2:] = np.max(np.concatenate([img_bev[:,2:], img_bev[:,:-2]]).reshape(2,configs.img_size, configs.img_size-2,3),0)\n",
    "\n",
    "targets = targets.reshape((-1,8))\n",
    "\n",
    "targets[:, 2:6] *= configs.img_size\n",
    "for targets_ in targets:\n",
    "    if targets_ is None:\n",
    "        continue\n",
    "    _, cls_pred, x, y, w, l, im, re = targets_\n",
    "    yaw = np.arctan2(im, re)\n",
    "    # Draw rotated box\n",
    "    bev_utils.drawRotatedBox(img_bev, x, y, w, l, yaw, [255,255,255])\n",
    "\n",
    "for detections in img_detections:\n",
    "    if detections is None:\n",
    "        continue\n",
    "    for x, y, w, l, im, re, *_, cls_pred in detections:\n",
    "        yaw = np.arctan2(im, re)\n",
    "        # Draw rotated box\n",
    "        bev_utils.drawRotatedBox(img_bev, x, y, w, l, yaw, [100,255,255])\n",
    "        \n",
    "img_rgb = cv2.imread(img_paths[0])\n",
    "\n",
    "img_bev = cv2.flip(cv2.flip(img_bev, 0), 1)\n",
    "out_img = img_bev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(out_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bev_image(model, batch_id):\n",
    "    img_paths, imgs_bev, targets = val_dataloader.dataset[batch_id]\n",
    "    \n",
    "    input_imgs = imgs_bev.to(device=configs.device).float()\n",
    "    input_imgs = torch.unsqueeze(input_imgs, dim=0)\n",
    "    targets = torch.unsqueeze(targets, dim=0)\n",
    "    \n",
    "    outputs = model(input_imgs)\n",
    "    detections = post_processing_v2(outputs, conf_thresh=configs.conf_thresh, nms_thresh=configs.nms_thresh)\n",
    "    \n",
    "    img_detections = []  # Stores detections for each image index\n",
    "    img_detections.extend(detections)\n",
    "\n",
    "    img_bev = imgs_bev.squeeze() * 255\n",
    "    img_bev = img_bev.permute(1, 2, 0).numpy().astype(np.uint8)\n",
    "    img_bev = cv2.resize(img_bev, (configs.img_size, configs.img_size))\n",
    "\n",
    "    img_bev[1:] = np.max(np.concatenate([img_bev[1:], img_bev[:-1]]).reshape(2,configs.img_size-1, configs.img_size,3),0)\n",
    "    img_bev[2:] = np.max(np.concatenate([img_bev[2:], img_bev[:-2]]).reshape(2,configs.img_size-2, configs.img_size,3),0)\n",
    "    img_bev[:,1:] = np.max(np.concatenate([img_bev[:,1:], img_bev[:,:-1]]).reshape(2,configs.img_size, configs.img_size-1,3),0)\n",
    "    img_bev[:,2:] = np.max(np.concatenate([img_bev[:,2:], img_bev[:,:-2]]).reshape(2,configs.img_size, configs.img_size-2,3),0)\n",
    "\n",
    "    targets = targets.reshape((-1,8))\n",
    "\n",
    "    targets[:, 2:6] *= configs.img_size\n",
    "    for targets_ in targets:\n",
    "        if targets_ is None:\n",
    "            continue\n",
    "        _, cls_pred, x, y, w, l, im, re = targets_\n",
    "        yaw = np.arctan2(im, re)\n",
    "        # Draw rotated box\n",
    "        bev_utils.drawRotatedBox(img_bev, x, y, w, l, yaw, [255,255,255])\n",
    "\n",
    "    for detections in img_detections:\n",
    "        if detections is None:\n",
    "            continue\n",
    "        for x, y, w, l, im, re, *_, cls_pred in detections:\n",
    "            yaw = np.arctan2(im, re)\n",
    "            # Draw rotated box\n",
    "            bev_utils.drawRotatedBox(img_bev, x, y, w, l, yaw, [100,255,255])\n",
    "\n",
    "    img_rgb = cv2.imread(img_paths[0])\n",
    "\n",
    "    img_bev = cv2.flip(cv2.flip(img_bev, 0), 1)\n",
    "    return img_bev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img = make_bev_image(model, batch_id)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(i):\n",
    "    print(i)\n",
    "    img = make_bev_image(model, i)\n",
    "    imshow = ax.imshow(img)\n",
    "    plt.title(f\"Frame {39+i}\")\n",
    "    return plt.imshow(img) ,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "frames = 91\n",
    "interval=1200\n",
    "repeat_delay = 3000\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "ax = plt.axes()\n",
    "\n",
    "anim = FuncAnimation(fig, update, frames = frames, interval=interval, blit=True, repeat_delay = repeat_delay)\n",
    "anim.save('low_fusion.gif', writer='imagemagick')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative Evaluation\n",
    "We calculate the average precision for the evaluatino set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "labels = []\n",
    "sample_metrics = []\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch_data in enumerate(val_dataloader):\n",
    "        img_path, imgs, targets = batch_data\n",
    "\n",
    "        labels += targets[:, 1].tolist()\n",
    "        # Rescale x, y, w, h of targets ((box_idx, class, x, y, w, l, im, re))\n",
    "        targets[:, 2:6] *= configs.img_size\n",
    "        imgs = imgs.to(configs.device, non_blocking=True)\n",
    "        \n",
    "        outputs = model(imgs)\n",
    "        outputs = post_processing_v2(outputs, conf_thresh=configs.conf_thresh, nms_thresh=configs.nms_thresh)\n",
    "        stats = get_batch_statistics_rotated_bbox(outputs, targets, iou_threshold=configs.iou_thresh)\n",
    "        sample_metrics += stats if stats else [[np.array([]), torch.tensor([]), torch.tensor([])]]\n",
    "    \n",
    "    true_positives, pred_scores, pred_labels = [np.concatenate(x, 0) for x in list(zip(*sample_metrics))]\n",
    "    precision, recall, AP, f1, ap_class = ap_per_class(true_positives, pred_scores, pred_labels, labels)\n",
    "end_time = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, AP, f1, ap_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "AP_list = []\n",
    "for epoch in range(10,300,10):\n",
    "    configs = edict({\n",
    "                    \"cfgfile\": \"./src/config/cfg/complex_yolov4.cfg\",\n",
    "                    \"dataset_dir\": \"../astyx/dataset/dataset_astyx_hires2019/\",\n",
    "                    \"pretrained_path\": f\"checkpoints/complex_yolov4_astyx_low_fusion_Mag_split_old/Model_complex_yolov4_astyx_low_fusion_Mag_split_old_epoch_{epoch}.pth\",\n",
    "                    \"radar\": False,\n",
    "                    \"low_fusion\": True,\n",
    "                    \"lidar\": False,\n",
    "                    \"VR\": False,\n",
    "                    \"mag\": True,\n",
    "                    \"img_size\": 608,\n",
    "                    \"conf_thresh\":0.5,\n",
    "                    \"nms_thresh\":0.5,\n",
    "                    \"iou_thresh\":0.5,\n",
    "                    \"batch_size\": 1,\n",
    "                    \"num_workers\": 1,\n",
    "                    \"pin_memory\": True,\n",
    "                    \"num_samples\": None,\n",
    "                    \"device\": torch.device('cpu'),\n",
    "                    \"arch\": \"darknet\",\n",
    "                    \"use_giou_loss\": True})\n",
    "    val_dataloader = create_val_dataloader(configs)\n",
    "    model = create_model(configs)\n",
    "    model.load_state_dict(torch.load(configs.pretrained_path, map_location=torch.device(configs.device)))\n",
    "    model = model.to(device=configs.device)\n",
    "    model.eval()\n",
    "\n",
    "    labels = []\n",
    "    sample_metrics = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch_data in enumerate(val_dataloader):\n",
    "            img_path, imgs, targets = batch_data\n",
    "\n",
    "            labels += targets[:, 1].tolist()\n",
    "            # Rescale x, y, w, h of targets ((box_idx, class, x, y, w, l, im, re))\n",
    "            targets[:, 2:6] *= configs.img_size\n",
    "            imgs = imgs.to(configs.device, non_blocking=True)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            outputs = post_processing_v2(outputs, conf_thresh=configs.conf_thresh, nms_thresh=configs.nms_thresh)\n",
    "            stats = get_batch_statistics_rotated_bbox(outputs, targets, iou_threshold=configs.iou_thresh)\n",
    "            sample_metrics += stats if stats else [[np.array([]), torch.tensor([]), torch.tensor([])]]\n",
    "\n",
    "        true_positives, pred_scores, pred_labels = [np.concatenate(x, 0) for x in list(zip(*sample_metrics))]\n",
    "        precision, recall, AP, f1, ap_class = ap_per_class(true_positives, pred_scores, pred_labels, labels)\n",
    "    AP_list += AP\n",
    "    print(epoch, AP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
